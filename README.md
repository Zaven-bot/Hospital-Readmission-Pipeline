
# Hospital Readmission Pipeline (ML + Airflow)

This project implements a complete machine learning pipeline for predicting hospital readmissions. It uses Apache Airflow to orchestrate tasks, MLflow to track model training, Docker for environment isolation, and DVC for data versioning.

---

## Project Summary

- **ML Tasks**: Data ingestion, cleaning, feature engineering, training, evaluation.
- **Orchestration**: Airflow DAG uses `BashOperator` to run each step in the ML pipeline.
- **Tracking**: MLflow logs metrics, parameters, and artifacts.
- **Versioning**: DVC tracks raw and processed datasets.
- **Environment Isolation**: Two Docker containers (`airflow`, `ml-pipeline`).
- **Service Management**: Docker Compose launches the full stack.

---

## Repository Layout

```
readmission-pipeline-ml/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ readmission_dag.py         # DAG defining ML pipeline orchestration
â”‚   â”œâ”€â”€ Dockerfile                     # Airflow service container
â”‚   â”œâ”€â”€ init.sh                        # DB init & graceful handling script
â”‚   â””â”€â”€ requirements.txt               # Python dependencies for Airflow
â”‚
â”œâ”€â”€ ml-pipeline/
â”‚   â”œâ”€â”€ Dockerfile                     # ML pipeline container
â”‚   â””â”€â”€ requirements.txt               # Python dependencies for ML tasks
â”‚
â”œâ”€â”€ scripts/                           # Modular ML pipeline components
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ clean.py
â”‚   â”œâ”€â”€ drop_column.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â””â”€â”€ pipeline.py                    # (Optional) End-to-end runner
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ hospital_readmissions.csv  # Raw source data
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ cleaned_data.csv
â”‚       â”œâ”€â”€ featured_data.csv
â”‚       â”œâ”€â”€ test_data.pkl
â”‚       â””â”€â”€ *.dvc                      # DVC tracking files
â”‚
â”œâ”€â”€ mlruns/                            # MLflow experiment logs & model registry
â”‚   â”œâ”€â”€ 0/, ...                        # Individual run folders
â”‚   â””â”€â”€ models/ReadmissionModel/...   # Model versions and metadata
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb                   # Exploratory Data Analysis
â”‚   â””â”€â”€ 02_cleaning.ipynb              # Cleaning prototyping
â”‚
â”œâ”€â”€ models/                            # (Optional) Local model storage
â”œâ”€â”€ docker-compose.yml                 # Defines all services
â”œâ”€â”€ Makefile                           # Manual build/run commands (optional)
â”œâ”€â”€ env.yml                            # Old Conda environment spec
â”œâ”€â”€ last_run_id.txt                    # Stores last MLflow run ID
â”œâ”€â”€ confusion_matrix.png               # Evaluation artifact
â”œâ”€â”€ roc_curve.png                      # Evaluation artifact
â””â”€â”€ README.md                          # You are here!
```

---

## ğŸ§  ML Pipeline Steps

Each ML step is executed in a Dockerized script:

1. **Ingest** â†’ `scripts/ingest.py`
2. **Clean** â†’ `scripts/clean.py`
3. **Feature Engineering** â†’ `scripts/feature_engineering.py`
4. **Train Model** â†’ `scripts/train_model.py`
5. **Evaluate** â†’ `scripts/evaluate_model.py`

Artifacts and metrics are logged to MLflow.

---

## âš™ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
      docker compose up --build
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow Container   â”‚    â”‚    ML Pipeline Containerâ”‚
â”‚   - DAG: bash steps   â”‚    â”‚    - run scripts        â”‚
â”‚   - Scheduler + UI    â”‚    â”‚    - log to MLflow      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â–¼                            â–¼
         PostgreSQL DB         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     (Airflow metadata store)  â”‚      MLflow UI       â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
```
```
```
```
```
```
```
```
```
```

                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚    Airflow Webserver     â”‚â—„â”€â”€ UI on :8080
                      â”‚  (DAG trigger + monitor) â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                Schedules / DAG Runs (e.g. daily)
                                   â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Airflow Scheduler     â”‚
                      â”‚  (Executes DAG tasks)   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                  â”‚                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ BashOperatorâ”‚      â”‚ BashOperatorâ”‚     â”‚ BashOperator â”‚
         â”‚ clean.py    â”‚      â”‚ feature_engineering.py â”‚  â”‚ train_model.py â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                  â”‚                  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼   â–¼   â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ evaluate_model.py  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each script runs inside:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Docker container: ml-pipeline         â”‚
â”‚  - Python 3.10                              â”‚
â”‚  - scikit-learn, pandas, joblib, etc.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All DAG logic is backed by:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PostgreSQL (airflow metadata DB)       â”‚
â”‚  - Tracks DAG runs, task states, logs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optional outputs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Local files & MLflow logs          â”‚
â”‚  - cleaned_data.csv, model.pkl, plots/     â”‚
â”‚  - mlruns/ for tracking experiments        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



---

## ğŸ§ª Local Development

```bash
# Stop & reset containers
docker compose down -v

# Rebuild everything
docker compose build

# Launch stack
docker compose up
```

---

## Next Steps

- Trigger DAG and run full pipeline from Airflow UI
- View metrics & artifacts in MLflow UI
- Add FASTAPI or Streamlit frontend
- (Future) Deploy to the cloud
